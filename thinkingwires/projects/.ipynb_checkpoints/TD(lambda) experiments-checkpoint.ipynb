{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "\n",
    "# create random walker to sample episodes\n",
    "\n",
    "# Monte Carlo\n",
    "\n",
    "# TD(0)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the MDP: [S, A, P, R, gamma]\n",
    "\n",
    "def getBasicRandomWalkMDP():\n",
    "    # set of states S:\n",
    "    S = ['A', 'B', 'C', 'D', 'E']\n",
    "    Sterminal = [0, 4]\n",
    "\n",
    "    # set of actions A:\n",
    "    A = ['left', 'right']\n",
    "\n",
    "    # state transition probability tensor P^a_s,s'\n",
    "    # P[action, from, to]\n",
    "    P = np.zeros((len(A), len(S), len(S)))\n",
    "    P[:,0,0] = 1\n",
    "    P[:,4,4] = 1\n",
    "    P[0,1,0] = 1\n",
    "    P[0,2,1] = 1\n",
    "    P[0,3,2] = 1\n",
    "    P[1,1,2] = 1\n",
    "    P[1,2,3] = 1\n",
    "    P[1,3,4] = 1\n",
    "#     print(\"state transitions for action 'left':\")\n",
    "#     print(P[0,:,:])\n",
    "#     print(\"state transitions for action 'right':\")\n",
    "#     print(P[1,:,:])\n",
    "\n",
    "    # reward function:\n",
    "    R = np.zeros((len(S), len(A)))\n",
    "    R[3,1] = 1\n",
    "#     print(\"reward function R[state,action]:\")\n",
    "#     print(R)\n",
    "\n",
    "    gamma = 0.9\n",
    "    \n",
    "    return {'S': S, 'Sterminal': Sterminal, 'A' : A, 'P' : P, 'R' : R, 'gamma' : gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRandomWalkSample(mdp):\n",
    "    currentState = np.random.randint(1, len(mdp['S']) - 1)\n",
    "    sampledStates = [currentState]\n",
    "    sampledRewards = []\n",
    "    while True:\n",
    "        sampledAction = np.random.randint(0,2)\n",
    "        transitionProbs = mdp['P'][sampledAction, currentState, :]\n",
    "        sampledState = np.where(np.random.multinomial(1, transitionProbs))[0][0]\n",
    "        sampledReward = mdp['R'][currentState, sampledAction]\n",
    "        sampledStates.append(sampledState)\n",
    "        sampledRewards.append(sampledReward)\n",
    "        currentState = sampledState\n",
    "        if sampledState in mdp['Sterminal']:\n",
    "            return sampledStates, sampledRewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.17210612  0.38571868  0.68474817  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# every visit implementation of Monte Carlo estimator:\n",
    "def getMonteCarloEstimator(mdp, numberOfEpisodes = 10000):\n",
    "    stateCounter = np.zeros(len(mdp['S']))\n",
    "    totalReturns = np.zeros(len(mdp['S']))\n",
    "    for i in range(0,numberOfEpisodes):\n",
    "        sampledStates, sampledRewards = getRandomWalkSample(mdp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update counter and returns for all states of episode (every visit)    \n",
    "        for i, state in enumerate(sampledStates):\n",
    "            stateCounter[state] += 1\n",
    "            \n",
    "            # estimate total time-discounted return G_t:\n",
    "            G_t = 0\n",
    "            for t, reward in enumerate(sampledRewards[i:]):\n",
    "                G_t += (mdp['gamma'] ** t) * reward\n",
    "            totalReturns[state] += G_t\n",
    "            \n",
    "    return totalReturns / stateCounter\n",
    "\n",
    "mdp = getBasicRandomWalkMDP()\n",
    "print(getMonteCarloEstimator(mdp, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.       0.17017  0.37815  0.67017  0.     ]\n"
     ]
    }
   ],
   "source": [
    "# solve value function analytically:\n",
    "\n",
    "# our policy is uniform random between 'left' and 'right':\n",
    "P = 0.5 * (mdp['P'][0,:,:] + mdp['P'][1,:,:])\n",
    "R = 0.5 * (mdp['R'][:,0] + mdp['R'][:,1])\n",
    "\n",
    "v = np.linalg.inv(np.identity(len(mdp['S'])) - mdp['gamma'] * P).dot(R)\n",
    "\n",
    "print(np.round(v, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TD(0) implementation:\n",
    "def TDzero(mdp, numberOfEpisodes):\n",
    "    currentVs = np.zeros(len(mdp['S']))\n",
    "    for i in range(0, numberOfEpisodes):\n",
    "        sampledStates, sampledRewards = getRandomWalkSample(mdp)\n",
    "        for state in sampledStates:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
