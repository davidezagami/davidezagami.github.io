{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "\n",
    "# create random walker to sample episodes\n",
    "\n",
    "# Monte Carlo\n",
    "\n",
    "# TD(0)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the MDP: [S, A, P, R, gamma]\n",
    "\n",
    "def getBasicRandomWalkMDP():\n",
    "    # set of states S:\n",
    "    S = ['A', 'B', 'C', 'D', 'E']\n",
    "    Sterminal = [0, 4]\n",
    "\n",
    "    # set of actions A:\n",
    "    A = ['left', 'right']\n",
    "\n",
    "    # state transition probability tensor P^a_s,s'\n",
    "    # P[action, from, to]\n",
    "    P = np.zeros((len(A), len(S), len(S)))\n",
    "    P[:,0,0] = 1\n",
    "    P[:,4,4] = 1\n",
    "    P[0,1,0] = 1\n",
    "    P[0,2,1] = 1\n",
    "    P[0,3,2] = 1\n",
    "    P[1,1,2] = 1\n",
    "    P[1,2,3] = 1\n",
    "    P[1,3,4] = 1\n",
    "#     print(\"state transitions for action 'left':\")\n",
    "#     print(P[0,:,:])\n",
    "#     print(\"state transitions for action 'right':\")\n",
    "#     print(P[1,:,:])\n",
    "\n",
    "    # reward function:\n",
    "    R = np.zeros((len(S), len(A)))\n",
    "    R[3,1] = 1\n",
    "#     print(\"reward function R[state,action]:\")\n",
    "#     print(R)\n",
    "\n",
    "    gamma = 0.9\n",
    "    \n",
    "    return {'S': S, 'Sterminal': Sterminal, 'A' : A, 'P' : P, 'R' : R, 'gamma' : gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRandomWalkSample(mdp):\n",
    "    currentState = np.random.randint(1, len(mdp['S']) - 1)\n",
    "    sampledStates = [currentState]\n",
    "    sampledRewards = []\n",
    "    while True:\n",
    "        sampledAction = np.random.randint(0,2)\n",
    "        transitionProbs = mdp['P'][sampledAction, currentState, :]\n",
    "        sampledState = np.where(np.random.multinomial(1, transitionProbs))[0][0]\n",
    "        sampledReward = mdp['R'][currentState, sampledAction]\n",
    "        sampledStates.append(sampledState)\n",
    "        sampledRewards.append(sampledReward)\n",
    "        currentState = sampledState\n",
    "        if sampledState in mdp['Sterminal']:\n",
    "            return sampledStates, sampledRewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.18698562  0.37756373  0.66999836  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# every visit implementation of Monte Carlo estimator:\n",
    "def getMonteCarloEstimator(mdp, numberOfEpisodes = 10000):\n",
    "    stateCounter = np.zeros(len(mdp['S']))\n",
    "    totalReturns = np.zeros(len(mdp['S']))\n",
    "    for i in range(0,numberOfEpisodes):\n",
    "        sampledStates, sampledRewards = getRandomWalkSample(mdp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update counter and returns for all states of episode (every visit)    \n",
    "        for i, state in enumerate(sampledStates):\n",
    "            stateCounter[state] += 1\n",
    "            \n",
    "            # estimate total time-discounted return G_t:\n",
    "            G_t = 0\n",
    "            for t, reward in enumerate(sampledRewards[i:]):\n",
    "                G_t += (mdp['gamma'] ** t) * reward\n",
    "            totalReturns[state] += G_t\n",
    "            \n",
    "    return totalReturns / stateCounter\n",
    "\n",
    "mdp = getBasicRandomWalkMDP()\n",
    "print(getMonteCarloEstimator(mdp, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.       0.17017  0.37815  0.67017  0.     ]\n"
     ]
    }
   ],
   "source": [
    "# solve value function analytically:\n",
    "\n",
    "# our policy is uniform random between 'left' and 'right':\n",
    "P = 0.5 * (mdp['P'][0,:,:] + mdp['P'][1,:,:])\n",
    "R = 0.5 * (mdp['R'][:,0] + mdp['R'][:,1])\n",
    "\n",
    "v = np.linalg.inv(np.identity(len(mdp['S'])) - mdp['gamma'] * P).dot(R)\n",
    "\n",
    "print(np.round(v, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.05015666  0.19285067  0.49593255  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# TD(0) implementation:\n",
    "def TDzero(mdp, numberOfEpisodes, alpha = 0.1):\n",
    "    currentVs = np.zeros(len(mdp['S']))\n",
    "    for i in range(0, numberOfEpisodes):\n",
    "        sampledStates, sampledRewards = getRandomWalkSample(mdp)\n",
    "        for j, reward in enumerate(sampledRewards):\n",
    "            state = sampledStates[j]\n",
    "            nextState = sampledStates[j+1]\n",
    "            TDtarget = reward + mdp['gamma'] * currentVs[nextState]\n",
    "            TDerror = TDtarget - currentVs[state]\n",
    "            currentVs[state] = currentVs[state] + alpha * TDerror\n",
    "    return currentVs\n",
    "\n",
    "Vs = TDzero(mdp, 100, 0.02)\n",
    "print(Vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.13328653  0.34370888  0.66850862  0.        ]\n"
     ]
    }
   ],
   "source": [
    "#TD(lambda) implementation:\n",
    "def TDlambda(mdp, numberOfEpisodes, alpha, lambd):\n",
    "    currentVs = np.zeros(len(mdp['S']))\n",
    "    for i in range(0, numberOfEpisodes):\n",
    "        sampledStates, sampledRewards = getRandomWalkSample(mdp)\n",
    "        eligibilityTraces = np.zeros(len(mdp['S']))\n",
    "        for j, reward in enumerate(sampledRewards):\n",
    "            state = sampledStates[j]\n",
    "            nextState = sampledStates[j+1]\n",
    "            eligibilityTraces = mdp['gamma'] * lambd * eligibilityTraces\n",
    "            eligibilityTraces[state] += 1\n",
    "            TDtarget = reward + mdp['gamma'] * currentVs[nextState]\n",
    "            TDerror = TDtarget - currentVs[state]\n",
    "            currentVs[state] = currentVs[state] + alpha * TDerror * eligibilityTraces[state]\n",
    "    return currentVs\n",
    "\n",
    "Vs = TDlambda(mdp, 100, 0.05, 0)\n",
    "print(Vs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.13640726  0.27233484  0.61650113  0.        ]\n",
      "[ 0.          0.13407358  0.29022057  0.65034728  0.        ]\n",
      "[ 0.          0.19086406  0.43765975  0.78520283  0.        ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "mdp = getBasicRandomWalkMDP()\n",
    "print(getMonteCarloEstimator(mdp, 100))\n",
    "print(TDzero(mdp, 100, 0.05))\n",
    "print(TDlambda(mdp, 100, 0.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [2_7_anaconda]",
   "language": "python",
   "name": "Python [2_7_anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
