<!DOCTYPE html>
<html>
<head>
	<title>Reward Function Domains | thinking wires</title>
	<meta charset="UTF-8">
	<link rel="stylesheet" href="../css/tufte-sansserif.css"/>
	<link rel="stylesheet" href="../css/thinkingwires.css"/>
	<link rel="stylesheet" href="../css/mailchimp-form.css"/>
	<link rel="stylesheet" href="../css/disqus.css"/>
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	  ga('create', 'UA-80489764-1', 'auto');
	  ga('send', 'pageview');
	</script>
	<script>
		// set constants for disqus comment section:
		var DISQUS_PERMA_URL = "https://thinkingwires.com/posts/2018-02-11-reward-function-domain.html";
		var DISQUS_IDENTIFIER = "posts/reward-function-domains";
	</script>

	<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>

</head>
<body>

<article>

	<noscript>This article contains math formulas rendered with <a href="https://www.mathjax.org">MathJax</a>. Please activate Javascript in your browser to see them</noscript>


	<section>
		<h1><a href='https://thinkingwires.com'>thinking wires</a></h1>
	</section>

	<section class="articleMenu">
		<!-- <p style="@media print {display:none !important}">
			<label for="mn-menu" class="margin-toggle">&#8853;</label>
	    <input type="checkbox" id="mn-menu" class="margin-toggle"/>
	    <span class="marginnote">
	      Jump to: <br />
	      <a href="#firstSection">First Section</a>
	      <br />
	      <a href="#secondSection">Second Section</a>
	      <br />
	      <a href="#thirdSection">Third Section</a>
	    </span> 
		</p> -->
		<h2>On the difference between $\mathcal{R}(s)$ and $\mathcal{R}(s, a)$</h2>
		<h4>Published 2018-02-11 by <a href="https://johannesheidecke.github.io" target="_blank">Johannes Heidecke</a></h4>
	</section>

	<section>
		<p>
			When reading recent papers on reinforcement learning (RL) or inverse reinforcement learning (IRL), each author uses a slightly different notation of MDPs. The biggest difference between them might be the reward function; sometimes it is defined as a function of states ($\mathcal{S} \rightarrow \mathbb{R}$) and other times it maps state-action pairs to rewards ($\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$).
		</p>
		<p>
			While this is of course not a huge difference, I think it is good to have an intution about how the two formulations differ and to know how one can change from one to the other. Most papers take this as a trivial prerequisite, such as Ng and Russel in their 2000 paper "Algorithms for Inverse Reinforcement Learning":
		</p>
		<blockquote>
			<p>"we have written rewards as $\mathcal{R}(s)$ rather than $\mathcal{R}(s, a)$; the extension is trivial."</p>
		</blockquote>
		<p>
			I found that many resources teaching RL strictly use one notation and do in fact not show how to switch to others. So for some of you this extension might not look all that trivial at first sight. Not to worry, after reading this post it should be quite obvious! 
		</p>
	</section>

	<section>
		<p>
			What is implied when using $\mathcal{R} : \mathcal{S} \rightarrow \mathbb{R}$?
		</p>
		<ul>
			<li>$\mathcal{R}(s)$ describes the reward upon achieving some state $s$. This is often the more natural description of a task, specifying which states are desirable</li>
			<li>$\mathcal{R}(s)$ can correspond to <em>actual</em> rewards or <em>sampled</em> rewards</li>
		</ul>

		<p>
			And with $\mathcal{R} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$?
		</p>
		<ul>
			<li>$\mathcal{R}(s, a)$ can be understood as the <em>expected</em> reward for performing action $a$ in state $s$: $\mathcal{R}(s, a) = \mathbb{E}_{s'\sim T(s, a)}[\mathcal{R}(s')]$</li>
			<li>This makes it harder notation-wise to use $\mathcal{R}(s, a)$ for actual or sampled rewards</li>
			<li>When using Q-learning or other methods based on the action-value function, it can be more natural to define $\mathcal{R}$ in this way</li>
		</ul>
		<p>
			In their standard RL book, Sutton and Barton state that while the MDP community often uses $\mathcal{R}(s, a)$, they prefer $\mathcal{R}(s)$ for reinforcement learning, since there "we more often have refer to individual actual or sample rewards (rather than just their expected values)."
		</p>
	</section>

	<section>
		<h3>
			Switching between notations
		</h3>
		<p>
			Switching between the two different notations is actually quite easy. To learn how, let's use a very basic and small MDP which is depicted in the following figure:
		</p>
		<figure>
	      <label for="mn-simple_mdp" class="margin-toggle">&#8853;</label>
	      <input type="checkbox" id="mn-simple_mdp" class="margin-toggle"/>
	      <span class="marginnote">
	      	A simple example MDP. There are three states ($s_0$, $s_1$, $s_2$) and two actions (<em>blue</em> and <em>orange</em>). The transitions are deterministic except for performing <em>blue</em> in $s_0$, with $T(s_1 | s_0, \text{blue})=40\%$ and $T(s_2 | s_0, \text{blue})=60\%$. For the color blind: blue action arrows are dotted lines. 
	      </span>
	      <img src="img/simple_mdp.png" alt="Diagram of expert estimates for the arrival of strong AI with different confidence levels" />
	    </figure>
	    <p>
	    	In the MDP shown above, we have three states and two actions: <em>blue / b</em> and <em>orange / o</em>. The only 'interesting' decision in this MDP occurs whenever the agent is in state $s_0$. It can choose to perform <em>o</em> to land in $s_2$ or alternatively use action <em>b</em> to land in $s_1$ with $40\%$ probability or end up in $s_2$ with $60\%$ probability.
	    </p>
	    <p>
	    	We are already given the rewards for each state (using the $\mathcal{S} \rightarrow \mathbb{R}$ formulation):
	    </p>
	    <table class="post-table">
	    	<tr class="header-row">
	    		<td>
	    			$\mathbf{s_i}$
	    		</td>
	    		<td>$\mathbf{\mathcal{R}(s_i)}$</td>
	    	</tr>
	    	<tr>
	    		<td>$s_0$</td>
	    		<td>$0$</td>
	    	</tr>
	    	<tr>
	    		<td>$s_1$</td>
	    		<td>$2$</td>
	    	</tr>
	    	<tr>
	    		<td>$s_2$</td>
	    		<td>$1$</td>
	    	</tr>
	    </table>
	</section>
	<section>
	    <p>
	    	<strong>From $\mathcal{R}(s)$ to $\mathcal{R}(s, a)$:</strong>
	    </p>
	    <p>
	    	Now, how can we turn this into a reward function for expected rewards of state action pairs? In order to do this, we need the transition probabilities $T$ for all state action pairs. Then we can simply calculate the value of any $\mathcal{R}(s,a)$ as the expected reward when performing action $a$ in state $s$:
	    	$$ \mathcal{R}(s, a) = \mathbb{E}_{s'\sim T(s, a)}[\mathcal{R}(s')] $$
	    </p>
	    <p>
	    	In our basic MDP example introduced above, let's do this for performing action <em>blue / b</em> in state $s_0$:
	    	$$
	    	\mathcal{R}(s_0, b) \\
			= T(s_0|s_0, b) * \mathcal{R}(s_0) +  T(s_1|s_0, b) * \mathcal{R}(s_1) + T(s_2|s_0, b) * \mathcal{R}(s_2)  \\
			= 0 \cdot \mathcal{R}(s_0)  + 0.4 \cdot \mathcal{R}(s_1)  + 0.6 \cdot \mathcal{R}(s_2)  \\
			= 0 \cdot 0  + 0.4 \cdot 2  + 0.6 \cdot1 \\
			= 1.4
	    	$$
	    </p>
	    <p>
	    	We can do this for the other state action pairs as well and arrive at the following results:
	    	$$
	    	\mathcal{R}(s_0, b) = 1.4 \\
	    	\mathcal{R}(s_0, o), \mathcal{R}(s_1, b), \mathcal{R}(s_1, o) = 1 \\
	    	\mathcal{R}(s_2, b), \mathcal{R}(s_2, o) = 0
	    	$$
	    </p>
	</section>
	<section>
		<p>
	    	<strong>From $\mathcal{R}(s, a)$ to $\mathcal{R}(s)$:</strong>
	    </p>
	    <p>
	    	Now that we have fully extracted $\mathcal{R}(s, a)$ using $\mathcal{R}(s)$ and the transition probabilities $T$, let's go the other direction. We already know that 
	    	$$ \mathcal{R}(s, a) = \mathbb{E}_{s'\sim T(s, a)}[\mathcal{R}(s')] $$
	    	Assuming that our state space is finite, this is equivalent to
	    	$$ \mathcal{R}(s, a) = \sum_{s'}T(s'| s, a) \cdot \mathcal{R}(s') $$
	    </p>
	    <p>
	    	Given that we know all $\mathcal{R}(s, a)$ and have full knowledge of the transition probabilities $T$, we can set up a system of linear equations for each unknown $\mathcal{R}(s')$, with the first three columns corresponding to $\mathcal{R}(s_0)$, $\mathcal{R}(s_1)$, and $\mathcal{R}(s_2)$ respectively:
	    	$$
	    	\left[
			\begin{array}{ccc|c}
			0 & 0.4 & 0.6 & 1.4 = \mathcal{R}(s_0, b) \\
			0 & 0 & 1 & 1 = \mathcal{R}(s_0, o) \\
			0 & 0 & 1 & 1 = \mathcal{R}(s_1, b) \\
			0 & 0 & 1 & 1 = \mathcal{R}(s_1, o)  \\
			1 & 0 & 0 & 0 = \mathcal{R}(s_2, b)  \\
			1 & 0 & 0 & 0 = \mathcal{R}(s_0, o) \\
			\end{array}
			\right]
	    	$$
	    </p>
	    <p>
	    	We can easily extract from the equations above that $\mathcal{R}(s_0) = 0$ and $\mathcal{R}(s_2) = 1$. From the first equation we can then find that $0.4\cdot \mathcal{R}(s_1) + 0.6 = 1.4$, so $\mathcal{R}(s_1) = 2$.
	    </p>
	    <p>
	    	Like this, we have successfully returned to our original reward formulation defined for each state. 
	    </p>
	</section>

	


	<section>
		<h2>Subscribe to Newsletter</h2>
		<p>Subscribe here to automatically receive new posts on thinking wires via email:</p>
		<!-- Begin MailChimp Signup Form -->
		<form action="//thinkingwires.us16.list-manage.com/subscribe/post?u=0ada73be2e3d8310377dc2cb8&amp;id=52de494c68" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
			<input id="mc-email" class="mc" type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
			<!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    	<div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_0ada73be2e3d8310377dc2cb8_52de494c68" tabindex="-1" value=""></div>
    	<br />
    	<input id="mc-submit" class="mc" type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe">
		</form>
		<!--End mc_embed_signup-->
	</section>

	<section>
		<p>
			<a href="https://thinkingwires.com">Back to main page</a>
		</p>
	</section>

	<section>
		<div id="disqus_thread"></div>
		<script>	
		var disqus_config = function () {
			this.page.url = DISQUS_PERMA_URL;  // Replace PAGE_URL with your page's canonical URL variable
			this.page.identifier = DISQUS_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
		};
		(function() { // DON'T EDIT BELOW THIS LINE
		var d = document, s = d.createElement('script');
		s.src = 'https://thinkingwires.disqus.com/embed.js';
		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
		})();
		</script>
		<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                      
	</section>
	
</article>

</body>
</html>